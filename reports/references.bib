@misc{blealtan2024blealtan,
  title = {Blealtan/Efficient-Kan},
  author = {Blealtan},
  year = {2024},
  month = jun,
  urldate = {2024-06-19},
  abstract = {An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN).},
  copyright = {MIT}
}

@misc{clanuwat2018deep,
  title = {Deep Learning for Classical Japanese Literature},
  author = {Clanuwat, Tarin and {Bober-Irizar}, Mikel and Kitamoto, Asanobu and Lamb, Alex and Yamamoto, Kazuaki and Ha, David},
  year = {2018},
  month = nov,
  eprint = {1812.01718},
  primaryclass = {cs, stat},
  doi = {10.20676/00000341},
  urldate = {2024-06-30},
  abstract = {Much of machine learning research focuses on producing models which perform well on benchmark tasks, in turn improving our understanding of the challenges associated with those tasks. From the perspective of ML researchers, the content of the task itself is largely irrelevant, and thus there have increasingly been calls for benchmark tasks to more heavily focus on problems which are of social or cultural relevance. In this work, we introduce Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Through these datasets, we wish to engage the machine learning community into the world of classical Japanese literature. Dataset available at https://github.com/rois-codh/kmnist},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{ioffe2015batch,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  number = {arXiv:1502.03167},
  eprint = {1502.03167},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.03167},
  urldate = {2024-06-30},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  langid = {american}
}

@article{lecun2010mnist,
  title = {{{MNIST}} Handwritten Digit Database},
  author = {LeCun, Yann and Cortes, Corinna and Burges, Cj},
  year = {2010},
  journal = {AT\&T Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume = {2},
  langid = {english}
}

@misc{li2024kolmogorov,
  title = {Kolmogorov-Arnold Networks Are Radial Basis Function Networks},
  author = {Li, Ziyao},
  year = {2024},
  month = may,
  number = {arXiv:2405.06721},
  eprint = {2405.06721},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.06721},
  urldate = {2024-06-19},
  abstract = {This short paper is a fast proof-of-concept that the 3-order B-splines used in Kolmogorov-Arnold Networks (KANs) can be well approximated by Gaussian radial basis functions. Doing so leads to FastKAN, a much faster implementation of KAN which is also a radial basis function (RBF) network.},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{liu2024kan,
  title = {{{KAN}}: Kolmogorov-Arnold Networks},
  shorttitle = {{{KAN}}},
  author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v c}i{\'c}, Marin and Hou, Thomas Y. and Tegmark, Max},
  year = {2024},
  month = may,
  number = {arXiv:2404.19756},
  eprint = {2404.19756},
  primaryclass = {cond-mat, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.19756},
  urldate = {2024-05-12},
  abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{xiao2017fashion,
  title = {Fashion-{{MNIST}}: A Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  shorttitle = {Fashion-{{MNIST}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = {2017},
  month = sep,
  number = {arXiv:1708.07747},
  eprint = {1708.07747},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.07747},
  urldate = {2024-06-30},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  archiveprefix = {arXiv},
  langid = {english}
}

@article{xu2015nonlinear,
  title = {Nonlinear Material Design Using Principal Stretches},
  author = {Xu, Hongyi and Sin, Funshing and Zhu, Yufeng and Barbi{\v c}, Jernej},
  year = {2015},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {34},
  number = {4},
  pages = {75:1--75:11},
  issn = {0730-0301},
  doi = {10.1145/2766917},
  urldate = {2024-06-19},
  abstract = {The Finite Element Method is widely used for solid deformable object simulation in film, computer games, virtual reality and medicine. Previous applications of nonlinear solid elasticity employed materials from a few standard families such as linear corotational, nonlinear St.Venant-Kirchhoff, Neo-Hookean, Ogden or Mooney-Rivlin materials. However, the spaces of all nonlinear isotropic and anisotropic materials are infinite-dimensional and much broader than these standard materials. In this paper, we demonstrate how to intuitively explore the space of isotropic and anisotropic nonlinear materials, for design of animations in computer graphics and related fields. In order to do so, we first formulate the internal elastic forces and tangent stiffness matrices in the space of the principal stretches of the material. We then demonstrate how to design new isotropic materials by editing a single stress-strain curve, using a spline interface. Similarly, anisotropic (orthotropic) materials can be designed by editing three curves, one for each material direction. We demonstrate that modifying these curves using our proposed interface has an intuitive, visual, effect on the simulation. Our materials accelerate simulation design and enable visual effects that are difficult or impossible to achieve with standard nonlinear materials.},
  langid = {english}
}
